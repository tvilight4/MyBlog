{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Final Evaluation Report for GSOC 2024","text":""},{"location":"#details","title":"Details","text":"Name Tvisha Vedant Organisation Neurobagel, INCF Mentor Arman Jahanpour, Sebastian Urchs,Alyssa Dai,Brent Mcpherson, Jean-Baptiste Poline  Project LLM-assisted tool to annotate research data with machine-understandable, semantic data dictionaries GitHub annotation-tool-ai Proposal Link Proposal"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>About Project</li> <li>Summary Of Work Done</li> <li>Contributions (PRs)</li> <li>Code That Didnt merge</li> <li>Future Plans</li> <li>Conclusion</li> </ul>"},{"location":"#about-project","title":"About Project","text":"<p>To participate in the Neurobagel query federation, datasets must conform to Neurobagel\u2019s data model, so annotating the datasets is necessary to harmonize them for query federation. The project aims to reduce the human effort required to manually annotate individual data elements by automating the current annotation tool tool provided by Neurobagel using Large Language Models (LLMs). The TSV files uploaded by the users get annotated by the LLMs to obtain the 'TermURLs' corresponding to each element, various other processing is carried out once the TermURL is obtained, and then the final JSON file with all the information of all the columns of the TSV file is generated, which can then be reviewed by a human.</p> <p>The project includes automating the annotation process using LLMs, integrating the tool into the existing webpage, and making changes in the UI accordingly.</p>"},{"location":"#summary-of-work-done","title":"Summary of Work Done","text":"<p>In the first week of our project, the main objective was to process the input TSV file and achieve two outcomes:</p> <pre><code>1)Generate a dictionary with key-value pairs representing column_header and a string of the column_header concatenated with the column entries.\n\n2)Create a JSON file with column headers as keys and empty value fields, which would be populated further in the process.\n</code></pre> <p>After completing the initial processing, we realized that the project should be divided into two main parts:</p> <pre><code>Parsing\nCategorization\n</code></pre> <p>As I was working with a colleague on this project, we divided our work. My primary focus was on the categorization aspect for quite some time before I started working on the React UI.</p> <p>For categorization, I used open-source LLMs from Ollama and leveraged various tools from LangChain. The goal was to make the LLM categorize columns (key-value pairs) into six categories:</p> <pre><code>Participant ID\nSession ID\nAge\nSex\nDiagnosis and \nAssessment Tool\n</code></pre> <p>The first milestone was successfully categorizing Participant ID, Session ID, Age, and Sex. Following this, we worked on categorizing Diagnosis and Assessment Tool.</p> <p>Working on the categorization part included the selection of the optimal model by testing the responses given by various models and the optimal utilization of the best possible tools from LangChain. The details of which can be found here.</p> <p>Prompt templates were extensively used and proved to be a critical component for the efficient functioning of the project. The prompt template for the first four categories included examples of inputs and how the LLM should respond. This approach worked well for the first four categories, but the LLM became confused when examples for Diagnosis and Assessment were included in the same prompt template. To address this, two additional prompt templates were created\u2014one for identifying Diagnosis and the other for Assessments. These templates included descriptions of the respective categories and instructions to return a \"yes\" or \"no,\" which was then used for categorization.</p> <p></p> <p>The response from the LLM was of the type \"AI_model_object,\" which needed to be converted into a string type for further processing as required by the parsing code to obtain the final result. The workflow of how a key-value pair is checked for its category is illustrated in the flowchart:</p> <p></p> <p>Tests were written using pytest, and the LLM responses were mocked because GitHub cannot directly call Ollama. The Pydantic library was extensively used to validate the values returned at various steps, ensuring the format and data types were in accordance with the requirements.</p> <p>The entire code was then dockerized. Once the Python script was fully functional, we developed a FastAPI service to run it, which was then integrated with a React frontend. The React frontend plays a crucial role as the final JSON with only a single diagnosis is created only when the user selects a specific diagnosis from the dropdown provided by the UI.</p> <p>When we were done with a fully functional Python script using Gemma, we also experimented with the OpenAI GPT-4 model, the key to which was provided by our organization for the purpose of studying and comparing the efficiencies of open-source models and closed models.</p> <p>To understand the details of the entire codebase, how the columns were analyzed for each individual category, and how the React UI aids in the diagnosis annotations, click here.</p> <p>During the entire course of the project, we documented our work and progress on HackMD and continuously updated the README of our GitHub repository.</p> <p>Towards the end of the project, we had the opportunity to present our project and work in front of the entire lab, show a live demo with the latest version of our tool, and address various questions regarding the tool we developed.</p>"},{"location":"#contributions","title":"Contributions","text":""},{"location":"#pull-requests-issued","title":"Pull Requests Issued","text":"<ul> <li>Create raw json with column headers</li> <li>Categorization of Participant_id, Session_id</li> <li>Categorization of Age and Sex Columns</li> <li>Adding Readme(This was continuosly updated by us)</li> <li>Assessment Descriptions</li> <li>Diagnosis Levels with correct API response</li> <li>UI development</li> </ul>"},{"location":"#code-that-did-not-get-merged","title":"Code That Did not get Merged","text":"<p>I had explored the use of vector stores for giving context to the LLM using LangChain to let the LLM predict the full form of diagnosis levels and identify whether a column is a diagnosis or an assessment tool, but the outputs were not very accurate, and the prompt template approach proved to be more efficient, so that was used for the final product.</p> <p>Also, our initial approach for diagnosis was for the LLM to predict the diagnosis levels' full form and then give the user an option to continue with it or replace it with a full form of their own choice from a dropdown. However, in order to take a more pragmatic approach, we decided to keep only the dropdown and eliminate that step.</p>"},{"location":"#future-plans","title":"Future Plans","text":"<ul> <li>Enhance the UI further to include more flexibility of usage for users, making it easier for carrying out the human check and updating the JSON file.</li> <li>Integrate the React app with the live annotation tool.</li> <li>Work on increasing the accuracy of the LLM by further enhancing the prompt templates.</li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<p>It has been an amazing experience for me, running into unforeseen errors and then solving them, learning the entire software cycle of writing production-level code, and learning about pytest and Docker. Everything has helped me gain new knowledge and developed my thought process in a way that will surely help me in my future endeavors. I am extremely grateful to my mentor Arman Jahanpour, Sebastian Urchs, Alyssa Dai, Brent McPherson, Neurobagel, and INCF for allowing me to work on the project and for all of their guidance and support. The tips and suggestions given by them whenever I got stuck always proved to be really helpful for me to progress further in the project. The insights shared by them helped me develop a unique point of view to approach problems. I would also like to thank Google for providing such a great opportunity to all open-source student developers across the globe.</p> <p>I am excited and look forward to contributing to Neurobagel and working with the organization on any future endeavors!!</p> <p>Thanks and Regards,</p> <p>Tvisha Vedant</p>"},{"location":"page2/","title":"LLM Utilisation- Annotation Tool AI Documentation","text":""},{"location":"page2/#categorizaton","title":"Categorizaton","text":""},{"location":"page2/#introduction","title":"Introduction","text":"<p>The codebase is designed to categorize/classify the columns present in the TSV input file into classes according to the existing categories present in the Neurobagel annotation tool, based on the column name and the content of the column. The LLM makes its predictions for a specific input string consisting of the column header and the column contents based on the examples provided to it beforehand in its prompt template. The various tasks carried out by this codebase mainly utilize LangChain, the json library from Python, and the LLM 'Gemma' from Ollama.</p>"},{"location":"page2/#important-aspects","title":"Important Aspects:","text":""},{"location":"page2/#1-choice-of-llm","title":"1. Choice of LLM","text":""},{"location":"page2/#2-utilising-langchain-and-a-well-structured-prompt-template","title":"2. Utilising Langchain and a well structured Prompt Template","text":""},{"location":"page2/#3-obtaining-the-output-in-the-req-format-to-pass-it-on-to-the-parser","title":"3. Obtaining the Output in the req format to pass it on to the parser:","text":""},{"location":"page2/#_1","title":"LLM Utilisation- Annotation Tool AI Documentation","text":""},{"location":"page2/#_2","title":"LLM Utilisation- Annotation Tool AI Documentation","text":""},{"location":"page2/#1choice-of-llm","title":"1.Choice of LLM:","text":"<p>The choice of LLM was a very crucial aspect as it was necessary to select an LLM that hallucinates the least and has the freedom to be used repeatedly without any limit.</p> <ol> <li>The LLMs from hugging face:</li> </ol> <p>Even though many LLMs like Flan-T5 are available for free on Hugging Face, it had an API request limit, and hence a conclusion was made that such an approach is not feasible for something to be sent to production. Therefore, we have been using LLMs from Ollama for the project.</p> <ol> <li>Llama2/Llama3:</li> </ol> <p>Even though Llama models appeared to work fine at first, hallucination was detected when tested further. The attached screenshots show the LLM response in the interval of about 10-20 seconds.</p> <p></p> <p></p> <ol> <li>Gemma Model: </li> </ol> <p>The Gemma model, also from Ollama, is working fine with no visible hallucinations so far. Further tasks are being carried out using the Gemma model.</p> <p>However, an open mind has been kept regarding the choice of LLM, and we keep testing new LLMs that we come across because there is always room for improvement in this context.</p> <ol> <li>Mistral/Mistral2:</li> </ol> <p>Mistral2 was launched midway through our project, and we did try it out. This model worked better than Gemma for the other project related to the query tool, but the outputs given by Gemma were better than those of the Mistral models for our project. One of the possibilities for this could be that the prompt templates developed by us work most optimally with Gemma but not with Mistral. Since we were getting good results with Gemma, we stuck with it.</p>"},{"location":"page2/#2-utilising-langchain-and-a-well-structured-prompt-template_1","title":"2. Utilising Langchain and a well structured Prompt Template","text":"<p>Utilizing LangChain\u2019s components like PromptTemplate, ChatOllama, and implementing chains provides a robust framework for developing and deploying our application.</p> <ol> <li>PromptTemplate:</li> </ol> <p>Here, the PromptTemplate specifies the input examples for the LLM to base its response on and the form in which the output is expected. It also separately defines the input variables that will be given to the LLM.</p> <p></p> <p></p> <ol> <li>Chatollama</li> </ol> <p>ChatOllama from langchain_community.chat_models was used to implement various LLMs from Ollama.</p> <p></p> <ol> <li>The Chain</li> </ol> <p>In LangChain, the concept of chains refers to a sequence or pipeline of operations applied to data. Chains in LangChain are flexible, allowing developers to incorporate various components (like ChatOllama, PromptTemplate, data parsers, etc.) into customized workflows. The chain.invoke(...) function executes each operation in the defined chain sequentially.</p> <p></p>"},{"location":"page2/#3-obtaining-the-output-in-the-req-format-to-pass-it-on-to-the-parser_1","title":"3. Obtaining the Output in the req format to pass it on to the parser","text":"<p>This depends on the type of values present in the column, i.e., categorical, defined numeric indices, continuous, etc.</p> <p>The LLM output is of the form of an 'AI_Model Object,' so it was required to convert it into a string object to further process the LLM output to obtain the required structured output from the code.</p> <p>Functions like SexLevel(...) and AgeFormat(...) are defined for different types of columns that require extra identification other than the labeling done by the LLM. Additionally, the output has been structured in a way that is required by the parser using the json.dumps() function.</p> <p>The key-value pairs that are not identified as \"Participant_ID\", \"Session_ID\", \"Age\", or \"Sex\" are then passed on to the llm_diagnosis_assessment function, where the column is categorized into its designated categories. The required structure to be sent to the parser code is obtained using the Diagnosis_Level and get_assessment_label functions for diagnosis and assessments, respectively.</p> <p></p>"},{"location":"page2/#basic-workflow","title":"Basic Workflow:","text":"<p>In the main processing.py script, the llm_invocation function is called for each individual pair of key-value corresponding to a column name and a string of column entries, respectively. The decision-making process followed for every input is as follows:</p> <p></p>"},{"location":"page3/","title":"Details of codebase and categorization for every category","text":"<p>The project was divided into 2 parts:</p> <p>1)Parsing</p> <p>2)Categorization</p>"},{"location":"page3/#parsing","title":"Parsing","text":"<p>This part was taken care of by my colleuge while I was working on the Categorization.</p>"},{"location":"page3/#categorization","title":"Categorization","text":"<p>Key-value pairs corresponding to the column header and a string consisting of the concatenation of the column header and column entries were used as a basis to classify each column.</p> <p>The prompt template for classifying Participant_ID, Session_ID, Age, and Sex was developed using input-output examples, referring to which the LLM could predict the category of each key-value pair input.</p> <p>The prompt template for assessments and diagnosis included descriptions of the respective tools, and the prompt included a question for the LLM to return a 'yes' or 'no' based on the reasoning if the key-value pairs fit the description. Based on the 'yes' and 'no' answer, the code returned the TermURL and other required information after further processing.</p> <p></p> <p>Diagnosis Levels</p> <p>The diagnosis levels were to be returned according to the full forms provided by the Neurobagel API for the corresponding acronyms. However, each acronym consisted of multiple full forms, and simply fetching the most probable full form wouldn't always have been correct. So instead, my colleague prepared a JSON file consisting of acronyms and their corresponding full forms from the existing Neurobagel API. I converted that JSON file into a dictionary with the acronym as the key and a list of corresponding full forms as the value field, as I noticed that the retrieval process was comparatively faster when using the process dictionary. Currently, once a column is identified as diagnosis, all the unique entries present in the column are listed, and then the full forms corresponding to each term are fetched from the dictionary. The output format is:</p> <pre><code>   {\n    \"TermURL\": \"nb:Diagnosis\",\n    \"Levels\": {\n        \"MDD\": [\"Major depressive disorder\",\" \"....],\n        \"HC\": [\"healthy control\",\"  \"...],\n        }\n   }\n\n</code></pre> <p>The user then needs to select one of the listed full forms from the entire list via a dropdown provided by the React UI, and a new updated JSON file is created with the single diagnosis selected by the user.</p> <p></p> <p>Test Code</p> <p>One of the things I learned during my GSOC journey is the importance of writing tests.</p> <p>To maintain code reliability, check if individual components are working correctly, and ensure that updating code doesn't hamper previous functionality, we made sure to add test codes for every new feature that we added.</p> <p>We used Pytest for writing our tests and mocked the LLM responses as GitHub does not run Ollama on its own. The test codes written by us were used for automating testing by implementing GitHub Continuous Integration Workflow.</p> <p>FastAPI integration and React UI</p> <p>My colleague worked on the development of the FastAPI backend. We went through the entire code using VSCode Liveshare and then later worked on the React UI. My colleague worked on the initial part of the React UI while I worked on the final parts of the Diagnosis, where I faced some issues related to Pydantic validations. Since the plan of returning a list was not decided earlier, there were a few declarations that had to be changed. I then worked on developing the dropdown and updating the JSON file according to the selection from the dropdown.</p>"}]}